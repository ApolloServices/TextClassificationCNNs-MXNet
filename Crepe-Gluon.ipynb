{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-level Convolutional Networks for text Classification\n",
    "\n",
    "## Crepe model implementation with MXNet/Gluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an implementation of [the crepe mode, Character-level Convolutional Networks for Text Classification](https://arxiv.org/abs/1509.01626) using the MXNet Gluon API. That this is the paper we reference throughout the tutorial\n",
    "\n",
    "We are going to perform a **text classification** task, trying to classify Amazon reviews according to the product category they belong to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Guide\n",
    "You need to install [Apache MXNet](http://mxnet.incubator.apache.org/) in order to run this tutorial. The following lines should work in most platform but checkout the [Apache install](http://mxnet.incubator.apache.org/install/index.html) guide for more info, especially if you plan to use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU install\n",
    "!pip install mxnet-cu90 pandas -q\n",
    "# CPU install\n",
    "#!pip install mxnet pandas -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data download\n",
    "The dataset has been made available on this website: http://jmcauley.ucsd.edu/data/amazon/, citation of relevant papers:\n",
    "\n",
    "**Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering**\n",
    "R. He, J. McAuley\n",
    "*WWW*, 2016\n",
    "\n",
    "**Image-based recommendations on styles and substitutes**\n",
    "J. McAuley, C. Targett, J. Shi, A. van den Hengel\n",
    "*SIGIR*, 2015\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are downloading a subset of the reviews, the k-core reviews, where k=5. That means that for each category, the dataset has been trimmed to only contain 5 reviews per individual product, and 5 reviews per user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Home_and_Kitchen\n",
      "Books\n",
      "CDs_and_Vinyl\n",
      "Movies_and_TV\n",
      "Cell_Phones_and_Accessories\n",
      "Sports_and_Outdoors\n",
      "Clothing_Shoes_and_Jewelry\n"
     ]
    }
   ],
   "source": [
    "base_url = 'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/'\n",
    "prefix = 'reviews_'\n",
    "suffix = '_5.json.gz'\n",
    "folder = 'data'\n",
    "categories = [\n",
    "    'Home_and_Kitchen', \n",
    "    'Books', \n",
    "    'CDs_and_Vinyl', \n",
    "    'Movies_and_TV', \n",
    "    'Cell_Phones_and_Accessories',\n",
    "    'Sports_and_Outdoors', \n",
    "    'Clothing_Shoes_and_Jewelry'\n",
    "]\n",
    "!mkdir -p $folder\n",
    "for category in categories:\n",
    "    print(category)\n",
    "    url = base_url+prefix+category+suffix\n",
    "    !wget -P $folder $url -nc -nv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing\n",
    "We need to perform some pre-processing steps in order to have the data in a format we can use for training (**X**,**Y**)\n",
    "In order to speed up training and balance the dataset we will only use a subset of reviews for each category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ITEMS_PER_CATEGORY = 250000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to read from the .json.gzip files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ec2-user/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ec2-user/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for line in g:\n",
    "        yield eval(line)\n",
    "\n",
    "def get_dataframe(path, num_lines):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        if i > num_lines:\n",
    "            break\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "\n",
    "    return pd.DataFrame.from_dict(df, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each category we load MAX_ITEMS_PER_CATEGORY by randomly sampling the files and shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data from file if exist\n",
    "try:\n",
    "    data = pd.read_pickle('pickleddata.pkl')\n",
    "except:\n",
    "    data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data is not available in the pickled file, we create it from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data is None:\n",
    "    data = pd.DataFrame(data={'X':[],'Y':[]})\n",
    "    for index, category in enumerate(categories):\n",
    "        df = get_dataframe(\"{}/{}{}{}\".format(folder, prefix, category, suffix), MAX_ITEMS_PER_CATEGORY)    \n",
    "        # Each review's summary is prepended to the main review text\n",
    "        df = pd.DataFrame(data={'X':(df['summary']+' | '+df['reviewText'])[:MAX_ITEMS_PER_CATEGORY],'Y':index})\n",
    "        data = data.append(df)\n",
    "        print('{}:{} reviews'.format(category, len(df)))\n",
    "\n",
    "    # Shuffle the samples\n",
    "    data = data.sample(frac=1)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    # Saving the data in a pickled file\n",
    "    pd.to_pickle(data, 'pickleddata.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts:\n",
      " 1.0    250000\n",
      "6.0    250000\n",
      "5.0    250000\n",
      "3.0    250000\n",
      "2.0    250000\n",
      "0.0    250000\n",
      "4.0    194439\n",
      "Name: Y, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why didnt I find this sooner!!! | This product...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The only thing weighing it down is the second ...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good | Works very good with a patch pulled or ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good mirror glasses | These are very reflectiv...</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cute, cushy, too small :( | Well, here's anoth...</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   X    Y\n",
       "0  Why didnt I find this sooner!!! | This product...  0.0\n",
       "1  The only thing weighing it down is the second ...  2.0\n",
       "2  Good | Works very good with a patch pulled or ...  5.0\n",
       "3  Good mirror glasses | These are very reflectiv...  6.0\n",
       "4  cute, cushy, too small :( | Well, here's anoth...  6.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Value counts:\\n',data['Y'].value_counts())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/urllib3/contrib/pyopenssl.py:46: DeprecationWarning: OpenSSL.rand is deprecated - you should use os.urandom instead\n",
      "  import OpenSSL.SSL\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon\n",
    "from mxnet.gluon.data import ArrayDataset\n",
    "from mxnet.gluon.data import DataLoader\n",
    "import numpy as np\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the parameters for the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHABET = list(\"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+ =<>()[]{}\") # The 69 characters as specified in the paper\n",
    "ALPHABET_INDEX = {letter: index for index, letter in enumerate(ALPHABET)} # { a: 0, b: 1, etc}\n",
    "FEATURE_LEN = 1014 # max-length in characters for one document\n",
    "BATCH_SIZE = 128 # number of documents per batch\n",
    "NUM_FILTERS = 256 # number of convolutional filters per convolutional layer\n",
    "NUM_OUTPUTS = len(categories) # number of classes\n",
    "FULLY_CONNECTED = 1024 # number of unit in the fully connected dense layer\n",
    "DROPOUT_RATE = 0.5 # probability of node drop out\n",
    "LEARNING_RATE = 0.01 # learning rate of the gradient\n",
    "MOMENTUM = 0.9 # momentum of the gradient\n",
    "WDECAY = 0.00001 # regularization term to limit size of weights\n",
    "NUM_WORKERS = multiprocessing.cpu_count() # number of workers used in the data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the paper, each document needs to be encoded in the following manner:\n",
    "    - Truncate to 1014 characters\n",
    "    - Reverse the string\n",
    "    - One-hot encode based on the alphabet\n",
    "    \n",
    "The following `encode` function does this for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "    encoded = np.zeros([len(ALPHABET), FEATURE_LEN], dtype='float32')\n",
    "    review = text.lower()[:FEATURE_LEN-1:-1]\n",
    "    i = 0\n",
    "    for letter in text:\n",
    "        if i >= FEATURE_LEN:\n",
    "            break;\n",
    "        if letter in ALPHABET_INDEX:\n",
    "            encoded[ALPHABET_INDEX[letter]][i] = 1\n",
    "        i += 1\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MXNet DataSet and DataLoader API lets you create different worker to pre-fetch the data and encode it the way you want, in order to prevent your GPU from starving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmazonDataSet(ArrayDataset):\n",
    "    # We pre-process the documents on the fly\n",
    "    def __getitem__(self, idx):\n",
    "        return encode(self._data[0][idx]), self._data[1][idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split our data into a training and a testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 0.8\n",
    "split_index = int(split*len(data)/BATCH_SIZE)*BATCH_SIZE\n",
    "train_data_X = data['X'][:split_index].as_matrix()\n",
    "train_data_Y = data['Y'][:split_index].as_matrix()\n",
    "test_data_X = data['X'][split_index:].as_matrix()\n",
    "test_data_Y = data['Y'][split_index:].as_matrix()\n",
    "train_dataset = AmazonDataSet(train_data_X, train_data_Y)\n",
    "test_dataset = AmazonDataSet(test_data_X, test_data_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the training and testing dataloader, with NUM_WORKERS set to the number of CPU core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The context will define where the training takes place, on the CPU or on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ctx = mx.cpu()\n",
    "ctx = mx.gpu() # to run on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the network following the instructions describe in the paper, using the small feature and small output units configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](data/diagram.png)\n",
    "![img](data/convolutional_layers.png)\n",
    "![img](data/dense_layer.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = gluon.nn.HybridSequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Conv1D(channels=NUM_FILTERS, kernel_size=7, activation='relu'))\n",
    "    net.add(gluon.nn.MaxPool1D(pool_size=3, strides=3))\n",
    "    net.add(gluon.nn.Conv1D(channels=NUM_FILTERS, kernel_size=7, activation='relu'))\n",
    "    net.add(gluon.nn.MaxPool1D(pool_size=3, strides=3))\n",
    "    net.add(gluon.nn.Conv1D(channels=NUM_FILTERS, kernel_size=3, activation='relu'))\n",
    "    net.add(gluon.nn.Conv1D(channels=NUM_FILTERS, kernel_size=3, activation='relu'))\n",
    "    net.add(gluon.nn.Conv1D(channels=NUM_FILTERS, kernel_size=3, activation='relu'))\n",
    "    net.add(gluon.nn.Conv1D(channels=NUM_FILTERS, kernel_size=3, activation='relu'))\n",
    "    net.add(gluon.nn.MaxPool1D(pool_size=3, strides=3))\n",
    "    net.add(gluon.nn.Flatten())\n",
    "    net.add(gluon.nn.Dense(FULLY_CONNECTED, activation='relu'))\n",
    "    net.add(gluon.nn.Dropout(DROPOUT_RATE))\n",
    "    net.add(gluon.nn.Dense(FULLY_CONNECTED, activation='relu'))\n",
    "    net.add(gluon.nn.Dropout(DROPOUT_RATE))\n",
    "    net.add(gluon.nn.Dense(NUM_OUTPUTS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HybridSequential(\n",
      "  (0): Conv1D(None -> 256, kernel_size=(7,), stride=(1,))\n",
      "  (1): MaxPool1D(size=(3,), stride=(3,), padding=(0,), ceil_mode=False)\n",
      "  (2): Conv1D(None -> 256, kernel_size=(7,), stride=(1,))\n",
      "  (3): MaxPool1D(size=(3,), stride=(3,), padding=(0,), ceil_mode=False)\n",
      "  (4): Conv1D(None -> 256, kernel_size=(3,), stride=(1,))\n",
      "  (5): Conv1D(None -> 256, kernel_size=(3,), stride=(1,))\n",
      "  (6): Conv1D(None -> 256, kernel_size=(3,), stride=(1,))\n",
      "  (7): Conv1D(None -> 256, kernel_size=(3,), stride=(1,))\n",
      "  (8): MaxPool1D(size=(3,), stride=(3,), padding=(0,), ceil_mode=False)\n",
      "  (9): Flatten\n",
      "  (10): Dense(None -> 1024, Activation(relu))\n",
      "  (11): Dropout(p = 0.5)\n",
      "  (12): Dense(None -> 1024, Activation(relu))\n",
      "  (13): Dropout(p = 0.5)\n",
      "  (14): Dense(None -> 7, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define whether we load a pre-trained version of the model and [hybridize the network](https://mxnet.incubator.apache.org/tutorials/gluon/hybrid.html) for speed improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybridize = True # for speed improvement, compile the network but no in-depth debugging possible\n",
    "load_params = True # Load pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_params:\n",
    "    net.load_params('crepe_gluon_epoch6.params', ctx=ctx)\n",
    "else:\n",
    "    net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybridization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hybridize:\n",
    "    net.hybridize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax cross-entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are in a multi-class classification problem, so we use the [Softmax Cross entropy loss](https://deepnotes.io/softmax-crossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', \n",
    "                        {'learning_rate': LEARNING_RATE, \n",
    "                         'wd':WDECAY, \n",
    "                         'momentum':MOMENTUM})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        prediction = nd.argmax(output, axis=1)\n",
    "\n",
    "        if (i%50 == 0):\n",
    "            print(\"Samples {}\".format(i*len(data)))\n",
    "        acc.update(preds=prediction, labels=label)\n",
    "    return acc.get()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "We loop through the batches given by the data_loader. These batches have been asynchronously fetched by the workers.\n",
    "\n",
    "After an epoch, we measure the test_accuracy and save the parameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples 0\n",
      "Samples 6400\n",
      "Samples 12800\n",
      "Samples 19200\n",
      "Samples 25600\n",
      "Samples 32000\n",
      "Samples 38400\n",
      "Samples 44800\n",
      "Samples 51200\n",
      "Samples 57600\n",
      "Samples 64000\n",
      "Samples 70400\n",
      "Samples 76800\n",
      "Samples 83200\n",
      "Samples 89600\n",
      "Samples 96000\n",
      "Samples 102400\n",
      "Samples 108800\n",
      "Samples 115200\n",
      "Samples 121600\n",
      "Samples 128000\n",
      "Samples 134400\n",
      "Samples 140800\n",
      "Samples 147200\n",
      "Samples 153600\n",
      "Samples 160000\n",
      "Samples 166400\n",
      "Samples 172800\n",
      "Samples 179200\n",
      "Samples 185600\n",
      "Samples 192000\n",
      "Samples 198400\n",
      "Samples 204800\n",
      "Samples 211200\n",
      "Samples 217600\n",
      "Samples 224000\n",
      "Samples 230400\n",
      "Samples 236800\n",
      "Samples 243200\n",
      "Samples 249600\n",
      "Samples 256000\n",
      "Samples 262400\n",
      "Samples 268800\n",
      "Samples 275200\n",
      "Samples 281600\n",
      "Samples 288000\n",
      "Samples 294400\n",
      "Samples 300800\n",
      "Samples 307200\n",
      "Samples 313600\n",
      "Samples 320000\n",
      "Samples 326400\n",
      "Samples 332800\n",
      "Epoch 6. Loss: 0.208511839838, Test_acc 0.928448980435\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 6\n",
    "number_epochs = 7\n",
    "smoothing_constant = .01\n",
    "for e in range(start_epoch, number_epochs):\n",
    "    for i, (review, label) in enumerate(train_dataloader):\n",
    "        review = review.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = net(review)\n",
    "        with autograd.record():\n",
    "            output = net(review)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(review.shape[0])\n",
    "        \n",
    "        # moving average of the loss\n",
    "        curr_loss = nd.mean(loss).asscalar()\n",
    "        moving_loss = (curr_loss if (i == 0) \n",
    "                       else (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss)\n",
    "\n",
    "        if (i%50 == 0):\n",
    "            nd.waitall()\n",
    "            print('Batch {}:{},{}'.format(i,curr_loss,moving_loss))\n",
    "\n",
    "    test_accuracy = evaluate_accuracy(test_dataloader, net)\n",
    "    #Save the model using the gluon params format\n",
    "    net.save_params('crepe_epoch_{}_test_acc_{}.params'.format(e,int(test_accuracy*10000)/100))\n",
    "    print(\"Epoch %s. Loss: %s, Test_acc %s\" % (e, moving_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to the symbolic format\n",
    "The `save_params()` method works for models trained in Gluon. \n",
    "\n",
    "However the `export()` function, exports it to a format usable in the symbolic API.\n",
    "We need the symbolic API in order to make it compatible with the current version of MXNet Model Server, for deployment purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.export('model/crepe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random testing\n",
    "\n",
    "Let's randomly pick a few reviews and see how the classifier does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Irreconcilable Similarities | There are several excellent books already in print by or about Richard M. Nixon and/or Henry A. Kissinger, notably Memoirs of Richard Nixon and Richard Reeves' President Nixon: Alone in the White House as well as Walter Isaacson's biography of Kissinger and The Kissinger Transcripts: The Top-Secret Talks With Beijing and Moscow. However, with access to a wealth of sources previously unavailable, Robert Dallek has written what will probably remain for quite some time the definitive study of one of U.S. history's most fascinating political partnerships.I defer to other reviewers to suggest parallels between the wars in Viet Nam and Iraq, especially when citing this passage in Dallek's Preface: \"Arguments about the wisdom of the war in Iraq and how to end the U.S. involvement there, relations with China and Russia, what to do about enduring Mideast trensions between Israelis and Arabs, and the advantages and disadvantages of an imperial presidency can, I believe, be usefully considered in the context of a fresh look ast Nixon and Kissinger and the power they wielded for good and ill.\"Until reading Dallek's book, I was unaware of the nature and extent of what Nixon and Kissinger shared in common. Of greatest interest to me was the almost total absence of trust in others (including each other) as, separately and together, they sought to increase their power, influence, and especially, their prestige. In countless ways, they were especially petty men and, when perceiving a threat, could be vindictive. They seemed to bring out the worst qualities in each other, as during their self-serving collaboration on policies \"good and ill\" in relationships with other countries such as China, Russia, Viet Nam, Pakistan, and Chile. Neither seemed to have must interest in domestic affairs (except for perceived threats to their respective careers) and Nixon once characterized them as \"building outhouses in Peoria.\"According to Dallek, \"Nixon's use of foreign affairs to overcome impeachment threats in 1973-1974 are a distubring part of the administration's history. Its impact on policy deserves particular consideration, as does the more extensive use of international relations to serve domestic political goals throughout Nixon's presidency. Nixon's competence to lead the country during his impeachment cruisis also requires the closest possible scrutiny.\"Most experts on this troubled period agree that the ceasefire agreement with North Viet Nam in 1973 was essentially the same as one that could have been concluded years before. However, both Nixon and Kissinger waited until after Nixon's re-election in1972 before ending a war that (by1966) Kissinger had characterized as \"unwinnable.\" According to Dallek, with access to 2,800 hours of Nixon tapes and 20,000 pages of Kissinger telephone transcripts, Kissinger would \"say almost anything privately to Nixon in the service of his ambition.\" Nixon referred to opponents of the war as \"communists.\" As the Watergate crisis intensified, Meanwhile, Kissinger conducted press briefings that were \"part reality, part fantasy, and part deception\" and referred to Democratic senators critical of the administration as \"traitors.\"Although they were in constant collaboration until Nixon's resignation, Nixon and Kissinger were never very close. Anti-Semitic elements in Nixon's personality have been well-documented and certainly had some influence on his attitude toward Kissinger. At one point, he recommended (through John Ehrlichman) that Kissinger needed psychiatric therapy and should obtain it. Kissinger frequently referred to Nixon as \"the meatball mind,\" \"our drunken friend,\" and \"That madman.\" It is certainly discomforting to realize that these two men, working together over a period of several years, made decisions and pursued policies that affected hundreds of millions of people throughout the world, \"for good and ill.\"I am now eager to read two other books (soon to be published) that may perhaps provide new insights and additional information about a political partnership that was probably doomed from the beginning because of so many irreconcilable similarities. Specifically Elizabeth Drew's Richard Nixon (part of \"The American Presidents\" series) and Jeremi Suri's Henry Kissinger and the American Century. However, I think Dallek's probing analysis will remain the definitive source of whatever can be known about these \"partners in power.\"\n",
      "Category: Books\n",
      "Correct\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "index = random.randint(1, len(data))\n",
    "review = data['X'][index]\n",
    "label = categories[int(data['Y'][index])]\n",
    "print(review)\n",
    "print('Category: {}'.format(label))\n",
    "encoded = nd.array([encode(review)], ctx=ctx)\n",
    "output = net(encoded)\n",
    "predicted = categories[np.argmax(output[0].asnumpy())]\n",
    "if predicted == label:\n",
    "      print('Correct')\n",
    "else:\n",
    "      print('Incorrectly predicted {}'.format(predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Testing\n",
    "We can also write our own reviews, encode them and see what the model predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_title = \"Good stuff\"\n",
    "review = \"This album is definitely better than the previous one\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good stuff\n",
      "This album is definitely better than the previous one\n",
      "\n",
      "Predicted: CDs_and_Vinyl\n",
      "\n",
      "Home_and_Kitchen 0.0 %\n",
      "Books 0.0 %\n",
      "CDs_and_Vinyl 98.7 %\n",
      "Movies_and_TV 0.8 %\n",
      "Cell_Phones_and_Accessories 0.2 %\n",
      "Sports_and_Outdoors 0.1 %\n",
      "Clothing_Shoes_and_Jewelry 0.0 %\n"
     ]
    }
   ],
   "source": [
    "print(review_title)\n",
    "print(review + '\\n')\n",
    "encoded = nd.array([encode(review + \" | \" + review_title)], ctx=ctx)\n",
    "output = net(encoded)\n",
    "softmax = nd.exp(output) / nd.sum(nd.exp(output))[0]\n",
    "predicted = categories[np.argmax(output[0].asnumpy())]\n",
    "print('Predicted: {}\\n'.format(predicted))\n",
    "for i, val in enumerate(categories):\n",
    "    print(val, float(int(softmax[0][i].asnumpy()*1000)/10), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Deployment\n",
    "\n",
    "Head over to the `model/` folder and have a look at the README.md to learn how you can deploy this pre-trained model to MXNet Model Server. You can then package the API in a docker container for cloud deployment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interactive live demo is available [here](https://thomasdelteil.github.io/CNN_NLP_MXNet/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![img](data/live_demo.png)](https://thomasdelteil.github.io/CNN_NLP_MXNet/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
