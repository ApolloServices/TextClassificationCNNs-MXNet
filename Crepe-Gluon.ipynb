{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crepe model implementation with MXNet/Gluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an implementation of [the crepe mode, Character-level Convolutional Networks for Text Classification](https://arxiv.org/abs/1509.01626) using the MXNet Gluon API.\n",
    "\n",
    "We are going to perform a **text classification** task, trying to classify Amazon reviews according to the product category they belong to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data download\n",
    "The dataset has been made available on this website: http://jmcauley.ucsd.edu/data/amazon/, citation of relevant papers:\n",
    "\n",
    "**Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering**\n",
    "R. He, J. McAuley\n",
    "*WWW*, 2016\n",
    "\n",
    "**Image-based recommendations on styles and substitutes**\n",
    "J. McAuley, C. Targett, J. Shi, A. van den Hengel\n",
    "*SIGIR*, 2015\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are downloading a subset of the reviews, the k-core reviews, where k=5. That means that for each category, the dataset has been trimmed to only contain 5 reviews per individual product, and 5 reviews per user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Home_and_Kitchen\n",
      "File ‘data/reviews_Home_and_Kitchen_5.json.gz’ already there; not retrieving.\r\n",
      "\r\n",
      "Books\n",
      "File ‘data/reviews_Books_5.json.gz’ already there; not retrieving.\r\n",
      "\r\n",
      "CDs_and_Vinyl\n",
      "File ‘data/reviews_CDs_and_Vinyl_5.json.gz’ already there; not retrieving.\r\n",
      "\r\n",
      "Movies_and_TV\n",
      "File ‘data/reviews_Movies_and_TV_5.json.gz’ already there; not retrieving.\r\n",
      "\r\n",
      "Cell_Phones_and_Accessories\n",
      "File ‘data/reviews_Cell_Phones_and_Accessories_5.json.gz’ already there; not retrieving.\r\n",
      "\r\n",
      "Sports_and_Outdoors\n",
      "File ‘data/reviews_Sports_and_Outdoors_5.json.gz’ already there; not retrieving.\r\n",
      "\r\n",
      "Clothing_Shoes_and_Jewelry\n",
      "File ‘data/reviews_Clothing_Shoes_and_Jewelry_5.json.gz’ already there; not retrieving.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "base_url = 'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/'\n",
    "prefix = 'reviews_'\n",
    "suffix = '_5.json.gz'\n",
    "folder = 'data'\n",
    "categories = [\n",
    "    'Home_and_Kitchen', \n",
    "    'Books', \n",
    "    'CDs_and_Vinyl', \n",
    "    'Movies_and_TV', \n",
    "    'Cell_Phones_and_Accessories',\n",
    "    'Sports_and_Outdoors', \n",
    "    'Clothing_Shoes_and_Jewelry'\n",
    "]\n",
    "!mkdir -p $folder\n",
    "for category in categories:\n",
    "    print(category)\n",
    "    url = base_url+prefix+category+suffix\n",
    "    !wget -P $folder $url -nc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing\n",
    "We need to perform some pre-processing steps in order to have the data in a format we can use for training (**X**,**Y**)\n",
    "In order to speed up training and balance the dataset we will only use a subset of reviews for each category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ITEMS_PER_CATEGORY = 250000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to read from the .json.gzip files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ec2-user/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ec2-user/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for line in g:\n",
    "        yield eval(line)\n",
    "\n",
    "def get_dataframe(path, num_lines):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        if i > num_lines:\n",
    "            break\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "\n",
    "    return pd.DataFrame.from_dict(df, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each category we load MAX_ITEMS_PER_CATEGORY by randomly sampling the files and shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data from file if exist\n",
    "try:\n",
    "    data = pd.read_pickle('pickleddata.pkl')\n",
    "except:\n",
    "    data = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data is None:\n",
    "    data = pd.DataFrame(data={'X':[],'Y':[]})\n",
    "    for index, category in enumerate(categories):\n",
    "        df = get_dataframe(\"{}/{}{}{}\".format(folder, prefix, category, suffix), MAX_ITEMS_PER_CATEGORY)    \n",
    "        # Each review's summary is prepended to the main review text\n",
    "        df = pd.DataFrame(data={'X':(df['summary']+' | '+df['reviewText'])[:MAX_ITEMS_PER_CATEGORY],'Y':index})\n",
    "        data = data.append(df)\n",
    "        print('{}:{} reviews'.format(category, len(df)))\n",
    "\n",
    "    # Shuffle the samples\n",
    "    data = data.sample(frac=1)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    # Saving the data in a pickled file\n",
    "    pd.to_pickle(data, 'pickleddata.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0    250000\n",
      "6.0    250000\n",
      "5.0    250000\n",
      "3.0    250000\n",
      "2.0    250000\n",
      "0.0    250000\n",
      "4.0    194439\n",
      "Name: Y, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Love these! | Love these for microwave use and...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Unholy Trinity of 1986, Part 1.  It's rain...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My favorite Disney Movie! | I love the scene w...</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dashboard Universal Car Mount holder - Perfect...</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great :) | Great for crop tops and other belly...</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   X    Y\n",
       "0  Love these! | Love these for microwave use and...  0.0\n",
       "1  The Unholy Trinity of 1986, Part 1.  It's rain...  2.0\n",
       "2  My favorite Disney Movie! | I love the scene w...  3.0\n",
       "3  Dashboard Universal Car Mount holder - Perfect...  4.0\n",
       "4  Great :) | Great for crop tops and other belly...  6.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data['Y'].value_counts())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon\n",
    "from mxnet.gluon.data import ArrayDataset\n",
    "from mxnet.gluon.data import DataLoader\n",
    "import numpy as np\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHABET = list(\"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+ =<>()[]{}\")\n",
    "ALPHABET_INDEX = {letter: index for index, letter in enumerate(ALPHABET)} # { a: 0, b: 1, etc}\n",
    "FEATURE_LEN = 1014 # max-length in characters for one document\n",
    "BATCH_SIZE = 128 # number of documents per batch\n",
    "NUM_FILTERS = 256 # number of convolutional filters per convolutional layer\n",
    "NUM_OUTPUTS = len(categories) # number of classes\n",
    "FULLY_CONNECTED = 1024 # number of unit in the fully connected dense layer\n",
    "DROPOUT_RATE = 0.5 # probability of node drop out\n",
    "LEARNING_RATE = 0.01 # learning rate of the gradient\n",
    "MOMENTUM = 0.9 # momentum of the gradient\n",
    "WDECAY = 0.00001 # regularization term to limit size of weights\n",
    "NUM_WORKERS = multiprocessing.cpu_count() # number of workers used in the data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "    encoded = np.zeros([len(ALPHABET), FEATURE_LEN], dtype='float32')\n",
    "    review = text.lower()[::-1]\n",
    "    i = 0\n",
    "    for letter in text:\n",
    "        if i >= FEATURE_LEN:\n",
    "            break;\n",
    "        if letter in ALPHABET_INDEX:\n",
    "            encoded[ALPHABET_INDEX[letter]][i] = 1\n",
    "        i += 1\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmazonDataSet(ArrayDataset):\n",
    "    # We pre-process the documents on the fly\n",
    "    def __getitem__(self, idx):\n",
    "        return encode(self._data[0][idx]), self._data[1][idx]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 0.8\n",
    "split_index = int(split*len(data)/BATCH_SIZE)*BATCH_SIZE\n",
    "train_dataset = AmazonDataSet(data['X'][:split_index].as_matrix(),data['Y'][:split_index].as_matrix())\n",
    "test_dataset = AmazonDataSet(data['X'][split_index:].as_matrix(),data['Y'][split_index:].as_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=BATCH_SIZE*6, num_workers=NUM_WORKERS*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.gpu() # use ctx = mx.cpu() to run on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = gluon.nn.HybridSequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Conv1D(channels=NUM_FILTERS, kernel_size=7, activation='relu'))\n",
    "    net.add(gluon.nn.MaxPool1D(pool_size=3, strides=3))\n",
    "    net.add(gluon.nn.Conv1D(channels=NUM_FILTERS, kernel_size=7, activation='relu'))\n",
    "    net.add(gluon.nn.MaxPool1D(pool_size=3, strides=3))\n",
    "    net.add(gluon.nn.Conv1D(channels=NUM_FILTERS, kernel_size=3, activation='relu'))\n",
    "    net.add(gluon.nn.Conv1D(channels=NUM_FILTERS, kernel_size=3, activation='relu'))\n",
    "    net.add(gluon.nn.Conv1D(channels=NUM_FILTERS, kernel_size=3, activation='relu'))\n",
    "    net.add(gluon.nn.Conv1D(channels=NUM_FILTERS, kernel_size=3, activation='relu'))\n",
    "    net.add(gluon.nn.MaxPool1D(pool_size=3, strides=3))\n",
    "    net.add(gluon.nn.Flatten())\n",
    "    net.add(gluon.nn.Dense(FULLY_CONNECTED, activation='relu'))\n",
    "    net.add(gluon.nn.Dropout(DROPOUT_RATE))\n",
    "    net.add(gluon.nn.Dense(FULLY_CONNECTED, activation='relu'))\n",
    "    net.add(gluon.nn.Dropout(DROPOUT_RATE))\n",
    "    net.add(gluon.nn.Dense(NUM_OUTPUTS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HybridSequential(\n",
      "  (0): Conv1D(None -> 256, kernel_size=(7,), stride=(1,))\n",
      "  (1): MaxPool1D(size=(3,), stride=(3,), padding=(0,), ceil_mode=False)\n",
      "  (2): Conv1D(None -> 256, kernel_size=(7,), stride=(1,))\n",
      "  (3): MaxPool1D(size=(3,), stride=(3,), padding=(0,), ceil_mode=False)\n",
      "  (4): Conv1D(None -> 256, kernel_size=(3,), stride=(1,))\n",
      "  (5): Conv1D(None -> 256, kernel_size=(3,), stride=(1,))\n",
      "  (6): Conv1D(None -> 256, kernel_size=(3,), stride=(1,))\n",
      "  (7): Conv1D(None -> 256, kernel_size=(3,), stride=(1,))\n",
      "  (8): MaxPool1D(size=(3,), stride=(3,), padding=(0,), ceil_mode=False)\n",
      "  (9): Flatten\n",
      "  (10): Dense(None -> 1024, Activation(relu))\n",
      "  (11): Dropout(p = 0.5)\n",
      "  (12): Dense(None -> 1024, Activation(relu))\n",
      "  (13): Dropout(p = 0.5)\n",
      "  (14): Dense(None -> 7, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybridize = True # for speed improvement, compile the network but no in-depth debugging possible\n",
    "load_params = True # Load pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_params:\n",
    "    net.load_params('crepe_gluon_epoch6.params', ctx=ctx)\n",
    "else:\n",
    "    net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybridization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hybridize:\n",
    "    net.hybridize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax cross-entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', \n",
    "                        {'learning_rate': LEARNING_RATE, \n",
    "                         'wd':WDECAY, \n",
    "                         'momentum':MOMENTUM})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        prediction = nd.argmax(output, axis=1)\n",
    "\n",
    "        if (i%50 == 0):\n",
    "            print(\"Samples {}\".formaat(i*len(data)))\n",
    "        acc.update(preds=prediction, labels=label)\n",
    "    return acc.get()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0:0.15787965059280396,0.15787965059280396\n",
      "Batch 50:0.10220436751842499,0.15457812894300896\n",
      "Batch 100:0.17303970456123352,0.15119215689855275\n",
      "Batch 150:0.2697896361351013,0.1522699015862132\n",
      "Batch 200:0.21811547875404358,0.1475157505321016\n",
      "Batch 250:0.09753967821598053,0.14239624025142694\n",
      "Batch 300:0.14830192923545837,0.14575528771982066\n",
      "Batch 350:0.18873190879821777,0.15026100733316544\n",
      "Batch 400:0.11522325873374939,0.14915754813674642\n",
      "Batch 450:0.15397216379642487,0.14515570012827422\n",
      "Batch 500:0.12619560956954956,0.13938639690421473\n",
      "Batch 0\n",
      "Batch 50\n",
      "Batch 100\n",
      "Batch 150\n",
      "Batch 200\n",
      "Batch 250\n",
      "Batch 300\n",
      "Batch 350\n",
      "Batch 400\n",
      "Epoch 6. Loss: 0.139386396904, Test_acc 0.932054561709\n"
     ]
    }
   ],
   "source": [
    "start_epoch = 6\n",
    "number_epochs = 7\n",
    "smoothing_constant = .01\n",
    "for e in range(start_epoch, number_epochs):\n",
    "    for i, (review, label) in enumerate(train_dataloader):\n",
    "        review = review.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = net(review)\n",
    "        with autograd.record():\n",
    "            output = net(review)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(review.shape[0])\n",
    "        \n",
    "        # moving average of the loss\n",
    "        curr_loss = nd.mean(loss).asscalar()\n",
    "        moving_loss = (curr_loss if (i == 0) \n",
    "                       else (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss)\n",
    "\n",
    "        if (i%50 == 0):\n",
    "            nd.waitall()\n",
    "            print('Batch {}:{},{}'.format(i,curr_loss,moving_loss))\n",
    "\n",
    "    test_accuracy = evaluate_accuracy(test_dataloader, net)\n",
    "    #Save the model using the gluon params format\n",
    "    net.save_params('crepe_epoch_{}_test_acc_{}.params'.format(e,int(test_accuracy*10000)/100))\n",
    "    print(\"Epoch %s. Loss: %s, Test_acc %s\" % (e, moving_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear the shared memory\n",
    "!rm -rf /dev/shm/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to the symbolic format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net.export('model/crepe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great Phones, great price! | I like these headphones a lot. The service was fast, the product came exactly when expected.Overall one does have to get used to the noise cancellation aspect of these phones, because it really does work. Aside from the adjustment to these, they work fantastic!\n",
      "Category: Cell_Phones_and_Accessories\n",
      "Correct\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "index = random.randint(1, len(data))\n",
    "review = data['X'][index]\n",
    "label = categories[int(data['Y'][index])]\n",
    "print(review)\n",
    "print('Category: {}'.format(label))\n",
    "encoded = nd.array([encode(review)], ctx=ctx)\n",
    "output = net(encoded)\n",
    "predicted = categories[np.argmax(output[0].asnumpy())]\n",
    "if predicted == label:\n",
    "      print('Correct')\n",
    "else:\n",
    "      print('Incorrectly predicted {}'.format(predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_title = \"Good stuff\"\n",
    "review = \"This album is definitely above the previous one\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good stuff\n",
      "This album is definitely above the previous one\n",
      "\n",
      "Predicted: CDs_and_Vinyl\n",
      "Home_and_Kitchen 0.0 %\n",
      "Books 0.0 %\n",
      "CDs_and_Vinyl 99.7 %\n",
      "Movies_and_TV 0.1 %\n",
      "Cell_Phones_and_Accessories 0.0 %\n",
      "Sports_and_Outdoors 0.0 %\n",
      "Clothing_Shoes_and_Jewelry 0.0 %\n"
     ]
    }
   ],
   "source": [
    "print(review_title)\n",
    "print(review + '\\n')\n",
    "encoded = nd.array([encode(review + \" | \" + review_title)], ctx=ctx)\n",
    "output = net(encoded)\n",
    "softmax = nd.exp(output) / nd.sum(nd.exp(output))[0]\n",
    "predicted = categories[np.argmax(output[0].asnumpy())]\n",
    "print('Predicted: {}'.format(predicted))\n",
    "for i, val in enumerate(categories):\n",
    "    print(val, float(int(softmax[0][i].asnumpy()*1000)/10), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_mxnet_p36)",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
