{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character-level Convolutional Networks for text Classification\n",
    "\n",
    "## Crepe model implementation with MXNet/Gluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an implementation of [the crepe model, Character-level Convolutional Networks for Text Classification](https://arxiv.org/abs/1509.01626) using the MXNet Gluon API. That this is the paper we reference throughout the tutorial\n",
    "\n",
    "We are going to perform a **text classification** task, trying to classify Amazon reviews according to the product category they belong to.\n",
    "\n",
    "This work is inspired from a previous collaborative work with [Ilia Karmanov and Miguel Fierro](https://github.com/ilkarman/NLP-Sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Guide\n",
    "You need to install [Apache MXNet](http://mxnet.incubator.apache.org/) in order to run this tutorial. The following lines should work in most platform but checkout the [Apache install](http://mxnet.incubator.apache.org/install/index.html) guide for more info, especially if you plan to use GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU install\n",
    "!pip install mxnet-cu90 pandas -q\n",
    "# CPU install\n",
    "#!pip install mxnet pandas -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data download\n",
    "The dataset has been made available on this website: http://jmcauley.ucsd.edu/data/amazon/, citation of relevant papers:\n",
    "\n",
    "**Ups and downs: Modeling the visual evolution of fashion trends with one-class collaborative filtering**\n",
    "R. He, J. McAuley\n",
    "*WWW*, 2016\n",
    "\n",
    "**Image-based recommendations on styles and substitutes**\n",
    "J. McAuley, C. Targett, J. Shi, A. van den Hengel\n",
    "*SIGIR*, 2015\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are downloading a subset of the reviews, the k-core reviews, where k=5. That means that for each category, the dataset has been trimmed to only contain 5 reviews per individual product, and 5 reviews per user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Home_and_Kitchen\n",
      "Books\n",
      "CDs_and_Vinyl\n",
      "Movies_and_TV\n",
      "Cell_Phones_and_Accessories\n",
      "Sports_and_Outdoors\n",
      "Clothing_Shoes_and_Jewelry\n"
     ]
    }
   ],
   "source": [
    "base_url = 'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/'\n",
    "prefix = 'reviews_'\n",
    "suffix = '_5.json.gz'\n",
    "folder = 'data'\n",
    "categories = [\n",
    "    'Home_and_Kitchen', \"\"\n",
    "    'Books', \n",
    "    'CDs_and_Vinyl', \n",
    "    'Movies_and_TV', \n",
    "    'Cell_Phones_and_Accessories',\n",
    "    'Sports_and_Outdoors', \n",
    "    'Clothing_Shoes_and_Jewelry'\n",
    "]\n",
    "!mkdir -p $folder\n",
    "for category in categories:\n",
    "    print(category)\n",
    "    url = base_url+prefix+category+suffix\n",
    "    !wget -P $folder $url -nc -nv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing\n",
    "We need to perform some pre-processing steps in order to have the data in a format we can use for training (**X**,**Y**)\n",
    "In order to speed up training and balance the dataset we will only use a subset of reviews for each category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_ITEMS_PER_CATEGORY = 250000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to read from the .json.gzip files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ec2-user/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/matplotlib/__init__.py:962: UserWarning: Duplicate key in file \"/home/ec2-user/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for line in g:\n",
    "        yield eval(line)\n",
    "\n",
    "def get_dataframe(path, num_lines):\n",
    "    i = 0\n",
    "    df = {}\n",
    "    for d in parse(path):\n",
    "        if i > num_lines:\n",
    "            break\n",
    "        df[i] = d\n",
    "        i += 1\n",
    "\n",
    "    return pd.DataFrame.from_dict(df, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each category we load MAX_ITEMS_PER_CATEGORY by randomly sampling the files and shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data from file if exist\n",
    "try:\n",
    "    data = pd.read_pickle('pickleddata.pkl')\n",
    "except:\n",
    "    data = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the data is not available in the pickled file, we create it from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if data is None:\n",
    "    data = pd.DataFrame(data={'X':[],'Y':[]})\n",
    "    for index, category in enumerate(categories):\n",
    "        df = get_dataframe(\"{}/{}{}{}\".format(folder, prefix, category, suffix), MAX_ITEMS_PER_CATEGORY)    \n",
    "        # Each review's summary is prepended to the main review text\n",
    "        df = pd.DataFrame(data={'X':(df['summary']+' | '+df['reviewText'])[:MAX_ITEMS_PER_CATEGORY],'Y':index})\n",
    "        data = data.append(df)\n",
    "        print('{}:{} reviews'.format(category, len(df)))\n",
    "\n",
    "    # Shuffle the samples\n",
    "    data = data.sample(frac=1)\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    # Saving the data in a pickled file\n",
    "    pd.to_pickle(data, 'pickleddata.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value counts:\n",
      " 1.0    250000\n",
      "6.0    250000\n",
      "5.0    250000\n",
      "3.0    250000\n",
      "2.0    250000\n",
      "0.0    250000\n",
      "4.0    194439\n",
      "Name: Y, dtype: int64\n",
      "0 Home_and_Kitchen\n",
      "1 Books\n",
      "2 CDs_and_Vinyl\n",
      "3 Movies_and_TV\n",
      "4 Cell_Phones_and_Accessories\n",
      "5 Sports_and_Outdoors\n",
      "6 Clothing_Shoes_and_Jewelry\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why didnt I find this sooner!!! | This product...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The only thing weighing it down is the second ...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good | Works very good with a patch pulled or ...</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Good mirror glasses | These are very reflectiv...</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cute, cushy, too small :( | Well, here's anoth...</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   X    Y\n",
       "0  Why didnt I find this sooner!!! | This product...  0.0\n",
       "1  The only thing weighing it down is the second ...  2.0\n",
       "2  Good | Works very good with a patch pulled or ...  5.0\n",
       "3  Good mirror glasses | These are very reflectiv...  6.0\n",
       "4  cute, cushy, too small :( | Well, here's anoth...  6.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Value counts:\\n',data['Y'].value_counts())\n",
    "for i,cat in enumerate(categories):\n",
    "    print(i, cat)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/lib/python3.6/site-packages/urllib3/contrib/pyopenssl.py:46: DeprecationWarning: OpenSSL.rand is deprecated - you should use os.urandom instead\n",
      "  import OpenSSL.SSL\n"
     ]
    }
   ],
   "source": [
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon\n",
    "from mxnet.gluon.data import ArrayDataset\n",
    "from mxnet.gluon.data import DataLoader\n",
    "import numpy as np\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the parameters for the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHABET = list(\"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+ =<>()[]{}\") # The 69 characters as specified in the paper\n",
    "ALPHABET_INDEX = {letter: index for index, letter in enumerate(ALPHABET)} # { a: 0, b: 1, etc}\n",
    "FEATURE_LEN = 1014 # max-length in characters for one document\n",
    "NUM_WORKERS = multiprocessing.cpu_count() # number of workers used in the data loading\n",
    "BATCH_SIZE = 128 # number of documents per batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the paper, each document needs to be encoded in the following manner:\n",
    "    - Truncate to 1014 characters\n",
    "    - Reverse the string\n",
    "    - One-hot encode based on the alphabet\n",
    "    \n",
    "The following `encode` function does this for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "    encoded = np.zeros([len(ALPHABET), FEATURE_LEN], dtype='float32')\n",
    "    review = text.lower()[:FEATURE_LEN-1:-1]\n",
    "    i = 0\n",
    "    for letter in text:\n",
    "        if i >= FEATURE_LEN:\n",
    "            break;\n",
    "        if letter in ALPHABET_INDEX:\n",
    "            encoded[ALPHABET_INDEX[letter]][i] = 1\n",
    "        i += 1\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MXNet DataSet and DataLoader API lets you create different worker to pre-fetch the data and encode it the way you want, in order to prevent your GPU from starving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmazonDataSet(ArrayDataset):\n",
    "    # We pre-process the documents on the fly\n",
    "    def __getitem__(self, idx):\n",
    "        return encode(self._data[0][idx]), self._data[1][idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split our data into a training and a testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 0.8\n",
    "split_index = int(split*len(data))\n",
    "train_data_X = data['X'][:split_index].as_matrix()\n",
    "train_data_Y = data['Y'][:split_index].as_matrix()\n",
    "test_data_X = data['X'][split_index:].as_matrix()\n",
    "test_data_Y = data['Y'][split_index:].as_matrix()\n",
    "train_dataset = AmazonDataSet(train_data_X, train_data_Y)\n",
    "test_dataset = AmazonDataSet(test_data_X, test_data_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the training and testing dataloader, with NUM_WORKERS set to the number of CPU core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, last_batch='discard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, shuffle=True, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS, last_batch='discard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The context will define where the training takes place, on the CPU or on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ctx = mx.cpu()\n",
    "ctx = mx.gpu() # to run on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the network following the instructions describe in the paper, using the small feature and small output units configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](data/diagram.png)\n",
    "![img](data/convolutional_layers.png)\n",
    "![img](data/dense_layer.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the paper we set the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FILTERS = 256 # number of convolutional filters per convolutional layer\n",
    "NUM_OUTPUTS = len(categories) # number of classes\n",
    "FULLY_CONNECTED = 1024 # number of unit in the fully connected dense layer\n",
    "DROPOUT_RATE = 0.5 # probability of node drop out\n",
    "LEARNING_RATE = 0.01 # learning rate of the gradient\n",
    "MOMENTUM = 0.9 # momentum of the gradient\n",
    "WDECAY = 0.00001 # regularization term to limit size of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = gluon.nn.HybridSequential()\n",
    "with net.name_scope():\n",
    "    net.add(gluon.nn.Conv1D(channels=NUM_FILTERS, kernel_size=7, activation='relu'))\n",
    "    net.add(gluon.nn.MaxPool1D(pool_size=3, strides=3))\n",
    "    net.add(gluon.nn.Conv1D(channels=NUM_FILTERS, kernel_size=7, activation='relu'))\n",
    "    net.add(gluon.nn.MaxPool1D(pool_size=3, strides=3))\n",
    "    net.add(gluon.nn.Conv1D(channels=NUM_FILTERS, kernel_size=3, activation='relu'))\n",
    "    net.add(gluon.nn.Conv1D(channels=NUM_FILTERS, kernel_size=3, activation='relu'))\n",
    "    net.add(gluon.nn.Conv1D(channels=NUM_FILTERS, kernel_size=3, activation='relu'))\n",
    "    net.add(gluon.nn.Conv1D(channels=NUM_FILTERS, kernel_size=3, activation='relu'))\n",
    "    net.add(gluon.nn.MaxPool1D(pool_size=3, strides=3))\n",
    "    net.add(gluon.nn.Flatten())\n",
    "    net.add(gluon.nn.Dense(FULLY_CONNECTED, activation='relu'))\n",
    "    net.add(gluon.nn.Dropout(DROPOUT_RATE))\n",
    "    net.add(gluon.nn.Dense(FULLY_CONNECTED, activation='relu'))\n",
    "    net.add(gluon.nn.Dropout(DROPOUT_RATE))\n",
    "    net.add(gluon.nn.Dense(NUM_OUTPUTS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HybridSequential(\n",
      "  (0): Conv1D(None -> 256, kernel_size=(7,), stride=(1,))\n",
      "  (1): MaxPool1D(size=(3,), stride=(3,), padding=(0,), ceil_mode=False)\n",
      "  (2): Conv1D(None -> 256, kernel_size=(7,), stride=(1,))\n",
      "  (3): MaxPool1D(size=(3,), stride=(3,), padding=(0,), ceil_mode=False)\n",
      "  (4): Conv1D(None -> 256, kernel_size=(3,), stride=(1,))\n",
      "  (5): Conv1D(None -> 256, kernel_size=(3,), stride=(1,))\n",
      "  (6): Conv1D(None -> 256, kernel_size=(3,), stride=(1,))\n",
      "  (7): Conv1D(None -> 256, kernel_size=(3,), stride=(1,))\n",
      "  (8): MaxPool1D(size=(3,), stride=(3,), padding=(0,), ceil_mode=False)\n",
      "  (9): Flatten\n",
      "  (10): Dense(None -> 1024, Activation(relu))\n",
      "  (11): Dropout(p = 0.5)\n",
      "  (12): Dense(None -> 1024, Activation(relu))\n",
      "  (13): Dropout(p = 0.5)\n",
      "  (14): Dense(None -> 7, linear)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define whether we load a pre-trained version of the model and [hybridize the network](https://mxnet.incubator.apache.org/tutorials/gluon/hybrid.html) for speed improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hybridize = True # for speed improvement, compile the network but no in-depth debugging possible\n",
    "load_params = True # Load pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_params:\n",
    "    net.load_params('crepe_gluon_epoch6.params', ctx=ctx)\n",
    "else:\n",
    "    net.collect_params().initialize(mx.init.Xavier(magnitude=2.24), ctx=ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hybridization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hybridize:\n",
    "    net.hybridize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax cross-entropy Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are in a multi-class classification problem, so we use the [Softmax Cross entropy loss](https://deepnotes.io/softmax-crossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_cross_entropy = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = gluon.Trainer(net.collect_params(), 'sgd', \n",
    "                        {'learning_rate': LEARNING_RATE, \n",
    "                         'wd':WDECAY, \n",
    "                         'momentum':MOMENTUM})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(data_iterator, net):\n",
    "    acc = mx.metric.Accuracy()\n",
    "    for i, (data, label) in enumerate(data_iterator):\n",
    "        data = data.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = net(data)\n",
    "        prediction = nd.argmax(output, axis=1)\n",
    "\n",
    "        if (i%50 == 0):\n",
    "            print(\"Samples {}\".format(i*len(data)))\n",
    "        acc.update(preds=prediction, labels=label)\n",
    "    return acc.get()[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "We loop through the batches given by the data_loader. These batches have been asynchronously fetched by the workers.\n",
    "\n",
    "After an epoch, we measure the test_accuracy and save the parameters of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 6\n",
    "number_epochs = 7\n",
    "smoothing_constant = .01\n",
    "for e in range(start_epoch, number_epochs):\n",
    "    for i, (review, label) in enumerate(train_dataloader):\n",
    "        review = review.as_in_context(ctx)\n",
    "        label = label.as_in_context(ctx)\n",
    "        output = net(review)\n",
    "        with autograd.record():\n",
    "            output = net(review)\n",
    "            loss = softmax_cross_entropy(output, label)\n",
    "        loss.backward()\n",
    "        trainer.step(review.shape[0])\n",
    "        \n",
    "        # moving average of the loss\n",
    "        curr_loss = nd.mean(loss).asscalar()\n",
    "        moving_loss = (curr_loss if (i == 0) \n",
    "                       else (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss)\n",
    "\n",
    "        if (i%50 == 0):\n",
    "            nd.waitall()\n",
    "            print('Batch {}:{},{}'.format(i,curr_loss,moving_loss))\n",
    "\n",
    "    test_accuracy = evaluate_accuracy(test_dataloader, net)\n",
    "    #Save the model using the gluon params format\n",
    "    net.save_params('crepe_epoch_{}_test_acc_{}.params'.format(e,int(test_accuracy*10000)/100))\n",
    "    print(\"Epoch %s. Loss: %s, Test_acc %s\" % (e, moving_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export to the symbolic format\n",
    "The `save_params()` method works for models trained in Gluon. \n",
    "\n",
    "However the `export()` function, exports it to a format usable in the symbolic API.\n",
    "We need the symbolic API in order to make it compatible with the current version of MXNet Model Server, for deployment purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.export('model/crepe')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random testing\n",
    "\n",
    "Let's randomly pick a few reviews and see how the classifier does!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine Breadmaker | We have used this mainly for the standard and whole wheat modes.  Their recipes work fine; also fine with Pamela's bread mix.\n",
      "\n",
      "Category: Home_and_Kitchen\n",
      "\n",
      "Correct\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "index = random.randint(1, len(data))\n",
    "review = data['X'][index]\n",
    "label = categories[int(data['Y'][index])]\n",
    "print(review)\n",
    "print('\\nCategory: {}\\n'.format(label))\n",
    "encoded = nd.array([encode(review)], ctx=ctx)\n",
    "output = net(encoded)\n",
    "predicted = categories[np.argmax(output[0].asnumpy())]\n",
    "if predicted == label:\n",
    "      print('Correct')\n",
    "else:\n",
    "      print('Incorrectly predicted {}'.format(predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Testing\n",
    "We can also write our own reviews, encode them and see what the model predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_title = \"Good stuff\"\n",
    "review = \"This album is definitely better than the previous one\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good stuff\n",
      "This album is definitely better than the previous one\n",
      "\n",
      "Predicted: CDs_and_Vinyl\n",
      "\n",
      "Home_and_Kitchen 0.0 %\n",
      "Books 0.0 %\n",
      "CDs_and_Vinyl 98.7 %\n",
      "Movies_and_TV 0.8 %\n",
      "Cell_Phones_and_Accessories 0.2 %\n",
      "Sports_and_Outdoors 0.1 %\n",
      "Clothing_Shoes_and_Jewelry 0.0 %\n"
     ]
    }
   ],
   "source": [
    "print(review_title)\n",
    "print(review + '\\n')\n",
    "encoded = nd.array([encode(review + \" | \" + review_title)], ctx=ctx)\n",
    "output = net(encoded)\n",
    "softmax = nd.exp(output) / nd.sum(nd.exp(output))[0]\n",
    "predicted = categories[np.argmax(output[0].asnumpy())]\n",
    "print('Predicted: {}\\n'.format(predicted))\n",
    "for i, val in enumerate(categories):\n",
    "    print(val, float(int(softmax[0][i].asnumpy()*1000)/10), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Deployment\n",
    "\n",
    "Head over to the `model/` folder and have a look at the README.md to learn how you can deploy this pre-trained model to MXNet Model Server. You can then package the API in a docker container for cloud deployment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interactive live demo is available [here](https://thomasdelteil.github.io/CNN_NLP_MXNet/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![img](data/live_demo.png)](https://thomasdelteil.github.io/CNN_NLP_MXNet/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
